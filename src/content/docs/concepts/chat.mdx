---
title: Chat
description: Learn about real-time streaming chat and session management with Astreus's agent-centric approach
---

The **Chat System** in Astreus 0.1.9+ is **agent-centric** and provides powerful **real-time streaming** capabilities. Instead of managing separate chat managers, everything flows through the agent with built-in session management.

## Agent-Centric Chat Architecture

With the new approach, chat functionality is built directly into agents:

```typescript
// ✅ New Agent-Centric Approach
const agent = await createAgent({
  name: 'ChatAgent',
  provider: myProvider,
  memory: myMemory
});

// Real-time streaming chat
await agent.streamChat({
  message: "Hello!",
  sessionId: "chat-123",
  onChunk: (chunk) => console.log(chunk)
});

// Standard chat
const response = await agent.chat({
  message: "How are you?",
  sessionId: "chat-123"
});

// Session management
const sessions = await agent.listSessions();
const history = await agent.getHistory("chat-123");
```

## Real-Time Streaming Chat

The core feature of the new chat system is **real-time streaming** for live responses:

### Basic Streaming

```typescript
import { createAgent, createProvider, createMemory, createDatabase } from '@astreus-ai/astreus';

async function setupStreamingAgent() {
  const db = await createDatabase();
  const memory = await createMemory({ database: db });
  const provider = createProvider({ 
    type: 'openai', 
    model: 'gpt-4o-mini',
    apiKey: process.env.OPENAI_API_KEY
  });

  return await createAgent({
    name: 'StreamingAssistant',
    provider: provider,
    memory: memory,
    systemPrompt: "You are a helpful assistant that provides detailed responses."
  });
}

// Use streaming chat
const agent = await setupStreamingAgent();

const response = await agent.streamChat({
  message: "Explain machine learning in simple terms",
  sessionId: "ml-discussion",
  temperature: 0.7,
  maxTokens: 1000,
  metadata: { topic: 'education', level: 'beginner' },
  onChunk: (chunk) => {
    // Each chunk arrives in real-time
    process.stdout.write(chunk);
    
    // Send to frontend via WebSocket
    // websocket.send(JSON.stringify({ type: 'chunk', content: chunk }));
  }
});

console.log('\nFull response:', response);
```

### WebSocket Integration

Here's how to integrate streaming with WebSocket for real-time frontend updates:

```typescript
import WebSocket from 'ws';
import { createAgent, createProvider, createMemory, createDatabase } from '@astreus-ai/astreus';

// WebSocket server for real-time chat
const wss = new WebSocket.Server({ port: 8080 });

async function createChatAgent() {
  const db = await createDatabase();
  const memory = await createMemory({ database: db });
  const provider = createProvider({ type: 'openai', model: 'gpt-4o-mini' });

  return await createAgent({
    name: 'WebSocketAgent',
    provider: provider,
    memory: memory,
    systemPrompt: "You are a helpful assistant for real-time chat."
  });
}

wss.on('connection', async (ws) => {
  const agent = await createChatAgent();
  
  ws.on('message', async (data) => {
    try {
      const { message, sessionId, userId } = JSON.parse(data.toString());
      
      // Stream response to client in real-time
      await agent.streamChat({
        message,
        sessionId: sessionId || `session_${userId}_${Date.now()}`,
        metadata: { userId, timestamp: new Date() },
        onChunk: (chunk) => {
          // Send each chunk immediately
          ws.send(JSON.stringify({ 
            type: 'chunk', 
            content: chunk,
            sessionId 
          }));
        }
      });
      
      // Signal completion
      ws.send(JSON.stringify({ 
        type: 'complete',
        sessionId 
      }));
      
    } catch (error) {
      ws.send(JSON.stringify({ 
        type: 'error', 
        message: error.message 
      }));
    }
  });
});

console.log('WebSocket chat server running on ws://localhost:8080');
```

## Standard Chat (Non-Streaming)

For scenarios where streaming isn't needed:

```typescript
// Standard chat without streaming
const response = await agent.chat({
  message: "What's the capital of France?",
  sessionId: "geography-quiz",
  temperature: 0.3,
  maxTokens: 100,
  metadata: { category: 'geography', difficulty: 'easy' }
});

console.log('Response:', response);
```

## Session Management

Agents provide built-in session management without needing separate chat managers:

### Session Operations

```typescript
// List all sessions for the agent
const sessions = await agent.listSessions(20);
sessions.forEach(session => {
  console.log(`Session: ${session.sessionId}`);
  console.log(`Messages: ${session.messageCount}`);
  console.log(`Last message: ${session.lastMessage}`);
  console.log(`Last activity: ${session.lastActivity}`);
  console.log(`Metadata:`, session.metadata);
});

// Get conversation history
const history = await agent.getHistory("session-123", 50);
console.log(`Found ${history.length} messages`);

// Add custom memory entries
await agent.addToMemory({
  sessionId: "session-123",
  role: 'system',
  content: "User prefers technical explanations",
  metadata: { preference: 'technical', timestamp: new Date() }
});

// Clear a session
await agent.clearHistory("old-session");
```

### Session Metadata

Sessions automatically track metadata and can be extended with custom data:

```typescript
// Chat with custom metadata
await agent.streamChat({
  message: "Help me with Python programming",
  sessionId: "python-help",
  metadata: {
    language: 'python',
    skill_level: 'intermediate',
    topic: 'programming',
    user_id: 'user123'
  },
  onChunk: (chunk) => console.log(chunk)
});

// Metadata is automatically stored and can be retrieved
const sessions = await agent.listSessions();
const pythonSession = sessions.find(s => s.sessionId === 'python-help');
console.log('Session metadata:', pythonSession?.metadata);
```

## Advanced Chat Patterns

### Context-Aware Conversations

Use memory search to provide context-aware responses:

```typescript
class ContextualAgent {
  private agent: AgentInstance;

  constructor(agent: AgentInstance) {
    this.agent = agent;
  }

  async contextualChat(message: string, sessionId: string) {
    // Search for relevant context from memory
    const memory = this.agent.config.memory;
    const relevantContext = await memory.searchByText(message, 3);
    
    // Build context-aware system prompt
    const contextPrompt = relevantContext.length > 0 
      ? `Context from previous conversations:\n${relevantContext.map(c => c.content).join('\n')}\n\nUser message: ${message}`
      : message;

    return await this.agent.streamChat({
      message: contextPrompt,
      sessionId,
      systemPrompt: `You are a helpful assistant. Use the provided context to give more relevant responses.`,
      onChunk: (chunk) => this.handleChunk(chunk)
    });
  }

  private handleChunk(chunk: string) {
    // Custom chunk processing
    console.log('Processing:', chunk);
  }
}
```

### Multi-Session Agent

Manage multiple conversations simultaneously:

```typescript
class MultiSessionAgent {
  private agent: AgentInstance;
  private activeSessions: Map<string, any> = new Map();

  constructor(agent: AgentInstance) {
    this.agent = agent;
  }

  async startSession(userId: string, topic?: string) {
    const sessionId = `${userId}_${Date.now()}`;
    
    this.activeSessions.set(sessionId, {
      userId,
      topic,
      startTime: new Date(),
      messageCount: 0
    });

    // Send welcome message
    await this.agent.streamChat({
      message: "GREETING_REQUEST",
      sessionId,
      metadata: { userId, topic, type: 'session_start' },
      onChunk: (chunk) => this.broadcastToUser(userId, chunk)
    });

    return sessionId;
  }

  async chatInSession(sessionId: string, message: string) {
    const session = this.activeSessions.get(sessionId);
    if (!session) {
      throw new Error('Session not found');
    }

    session.messageCount++;
    
    return await this.agent.streamChat({
      message,
      sessionId,
      metadata: { 
        userId: session.userId, 
        messageNumber: session.messageCount 
      },
      onChunk: (chunk) => this.broadcastToUser(session.userId, chunk)
    });
  }

  async getSessionSummary(sessionId: string) {
    const history = await this.agent.getHistory(sessionId);
    const session = this.activeSessions.get(sessionId);
    
    return {
      sessionId,
      messageCount: history.length,
      duration: session ? Date.now() - session.startTime.getTime() : 0,
      lastMessage: history[history.length - 1]?.content,
      metadata: session
    };
  }

  private broadcastToUser(userId: string, chunk: string) {
    // Implement WebSocket broadcasting to specific user
    console.log(`To ${userId}:`, chunk);
  }
}
```

### Streaming with Progress Tracking

Track streaming progress and provide feedback:

```typescript
class ProgressTrackingAgent {
  private agent: AgentInstance;

  constructor(agent: AgentInstance) {
    this.agent = agent;
  }

  async streamWithProgress(message: string, sessionId: string, onProgress?: (progress: number) => void) {
    let totalChunks = 0;
    let processedChunks = 0;
    const chunks: string[] = [];

    const response = await this.agent.streamChat({
      message,
      sessionId,
      onChunk: (chunk) => {
        totalChunks++;
        chunks.push(chunk);
        
        // Estimate progress (rough approximation)
        const estimatedTotal = Math.max(totalChunks * 1.2, 10);
        const progress = Math.min((totalChunks / estimatedTotal) * 100, 95);
        
        if (onProgress) {
          onProgress(progress);
        }
        
        console.log(`Progress: ${progress.toFixed(1)}% - Chunk: ${chunk}`);
      }
    });

    // Final progress
    if (onProgress) {
      onProgress(100);
    }

    return {
      response,
      chunks,
      totalChunks,
      averageChunkSize: chunks.join('').length / chunks.length
    };
  }
}
```

## Error Handling and Resilience

Implement robust error handling for streaming chat:

```typescript
class ResilientAgent {
  private agent: AgentInstance;
  private maxRetries = 3;

  constructor(agent: AgentInstance) {
    this.agent = agent;
  }

  async resilientStreamChat(message: string, sessionId: string, onChunk?: (chunk: string) => void) {
    let attempt = 0;
    
    while (attempt < this.maxRetries) {
      try {
        return await this.agent.streamChat({
          message,
          sessionId,
          onChunk: (chunk) => {
            try {
              if (onChunk) onChunk(chunk);
            } catch (chunkError) {
              console.error('Chunk processing error:', chunkError);
            }
          }
        });
      } catch (error) {
        attempt++;
        console.error(`Attempt ${attempt} failed:`, error);
        
        if (attempt >= this.maxRetries) {
          // Fallback to non-streaming
          console.log('Falling back to non-streaming chat');
          return await this.agent.chat({ message, sessionId });
        }
        
        // Wait before retry
        await new Promise(resolve => setTimeout(resolve, 1000 * attempt));
      }
    }
  }
}
```

## Migration from Chat Managers

If you're migrating from the old chat manager approach:

```typescript
// ❌ Old Chat Manager Approach
const chatManager = await createChat({ database, memory });
const response = await chatManager.chat({
  chatId,
  agentId,
  userId,
  message,
  provider,
  systemPrompt
});

// ✅ New Agent-Centric Approach
const agent = await createAgent({ provider, memory });
const response = await agent.streamChat({
  message,
  sessionId: chatId, // chatId becomes sessionId
  systemPrompt,
  onChunk: (chunk) => console.log(chunk) // Real-time streaming
});
```

## Best Practices

1. **Use Streaming**: Always prefer `streamChat()` for better user experience
2. **Consistent Session IDs**: Use meaningful, consistent session identifiers
3. **Error Handling**: Implement proper error handling for network issues
4. **Progress Feedback**: Show streaming progress to users
5. **Memory Management**: Regularly clean old sessions to manage storage
6. **Metadata Usage**: Store relevant metadata for session organization
7. **WebSocket Integration**: Use WebSockets for real-time frontend updates
8. **Fallback Strategy**: Have non-streaming fallback for error scenarios

The agent-centric chat system provides a more streamlined, powerful approach to conversational AI with real-time capabilities built-in. 