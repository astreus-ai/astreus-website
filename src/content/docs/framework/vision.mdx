---
title: Vision
icon: Eye
---

![Vision](/docs/vision.webp)

**Image analysis and document processing for multimodal interactions**

import { Step, Steps } from 'fumadocs-ui/components/steps';

## Overview

The Vision system enables agents to process and analyze images, providing multimodal AI capabilities for richer interactions. It supports multiple image formats, offers various analysis modes, and integrates seamlessly with both OpenAI and local Ollama providers for flexible deployment options.

## Enabling Vision

Enable vision capabilities for an agent by setting the `vision` option to `true`:

```typescript
import { Agent } from '@astreus-ai/astreus';

const agent = await Agent.create({
  name: 'VisionAgent',
  model: 'gpt-4o',  // Vision-capable model
  vision: true      // Enable vision capabilities (default: false)
});
```

## Attachment System

Astreus supports an intuitive attachment system for working with images:

```typescript
// Clean, modern attachment API
const response = await agent.ask("What do you see in this image?", {
  attachments: [
    { type: 'image', path: '/path/to/image.jpg', name: 'My Photo' }
  ]
});
```

The attachment system automatically:
- Detects the file type and selects appropriate tools
- Enhances the prompt with attachment information
- Enables tool usage when attachments are present

## Vision Capabilities

The vision system provides three core capabilities through built-in tools:

### 1. General Image Analysis
Analyze images with custom prompts and configurable detail levels:

```typescript
// Using attachments (recommended approach)
const response = await agent.ask("Please analyze this screenshot and describe the UI elements", {
  attachments: [
    { type: 'image', path: '/path/to/screenshot.png', name: 'UI Screenshot' }
  ]
});

// Using the analyze_image tool through conversation
const response2 = await agent.ask("Please analyze the image at /path/to/screenshot.png and describe the UI elements");

// Direct method call
const analysis = await agent.analyzeImage('/path/to/image.jpg', {
  prompt: 'What UI elements are visible in this interface?',
  detail: 'high',
  maxTokens: 1500
});
```

### 2. Image Description
Generate structured descriptions for different use cases:

```typescript
// Accessibility-friendly description
const description = await agent.describeImage('/path/to/image.jpg', 'accessibility');

// Available styles:
// - 'detailed': Comprehensive description of all visual elements
// - 'concise': Brief description of main elements  
// - 'accessibility': Screen reader-friendly descriptions
// - 'technical': Technical analysis including composition and lighting
```

### 3. Text Extraction (OCR)
Extract and transcribe text from images:

```typescript
// Extract text with language hint
const text = await agent.extractTextFromImage('/path/to/document.jpg', 'english');

// The system maintains original formatting and structure
console.log(text);
```

## Supported Formats

The vision system supports these image formats:

- **JPEG** (`.jpg`, `.jpeg`)
- **PNG** (`.png`)
- **GIF** (`.gif`)
- **BMP** (`.bmp`)
- **WebP** (`.webp`)

### Input Sources

<Steps>
<Step>
### File Paths
Analyze images from local file system:

```typescript
const result = await agent.analyzeImage('/path/to/image.jpg');
```
</Step>

<Step>
### Base64 Data
Analyze images from base64-encoded data:

```typescript
const base64Image = 'data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQ...';
const result = await agent.analyzeImageFromBase64(base64Image);
```
</Step>
</Steps>

## Configuration

### Environment Variables

```bash
# Provider selection
VISION_PROVIDER=openai  # Default: 'openai' if not set
VISION_MODEL=gpt-4o     # Default: 'gpt-4o' for OpenAI, 'llava' for Ollama

# OpenAI configuration  
OPENAI_VISION_API_KEY=your_openai_key  # Optional, falls back to OPENAI_API_KEY
OPENAI_API_KEY=your_openai_key         # Used if OPENAI_VISION_API_KEY not set

# Ollama configuration (local)
OLLAMA_BASE_URL=http://localhost:11434  # Default if not set
```

### Vision Configuration

Configure the vision service with these options:

```typescript
interface VisionConfig {
  provider: 'openai' | 'ollama';  // Vision provider
  model?: string;                  // Model to use
  apiKey?: string;                 // API key for provider
  baseURL?: string;                // Custom base URL (OpenAI only)
}
```

### Analysis Options

Configure analysis behavior with these options:

```typescript
interface AnalysisOptions {
  prompt?: string;           // Custom analysis prompt
  maxTokens?: number;        // Response length limit (default: 1000)
  detail?: 'low' | 'high';   // Analysis detail level (OpenAI only)
}
```

## Usage Examples

### Screenshot Analysis

```typescript
const agent = await Agent.create({
  name: 'UIAnalyzer',
  model: 'gpt-4o',
  vision: true
});

// Analyze a UI screenshot
const analysis = await agent.analyzeImage('/path/to/app-screenshot.png', {
  prompt: 'Analyze this mobile app interface. Identify key UI components, layout structure, and potential usability issues.',
  detail: 'high'
});

console.log(analysis);
```

### Document Processing

```typescript
// Extract text from scanned documents
const documentText = await agent.extractTextFromImage('/path/to/scanned-invoice.jpg', 'english');

// Generate accessible descriptions
const accessibleDesc = await agent.describeImage('/path/to/chart.png', 'accessibility');
```

### Multimodal Conversations

```typescript
// Using attachments for cleaner API
const response = await agent.ask("I'm getting an error. Can you analyze this screenshot and help me fix it?", {
  attachments: [
    { type: 'image', path: '/Users/john/Desktop/error.png', name: 'Error Screenshot' }
  ]
});

// Multiple attachments
const response2 = await agent.ask("Compare these UI mockups and suggest improvements", {
  attachments: [
    { type: 'image', path: '/designs/mockup1.png', name: 'Design A' },
    { type: 'image', path: '/designs/mockup2.png', name: 'Design B' }
  ]
});

// Traditional approach (still works)
const response3 = await agent.ask(
  "Please analyze the error screenshot at /Users/john/Desktop/error.png and suggest how to fix the issue"
);
```

## Provider Comparison

| Feature | OpenAI (gpt-4o) | Ollama (llava) |
|---------|-----------------|----------------|
| Analysis Quality | Excellent | Good |
| Processing Speed | Fast | Variable |
| Cost | Pay-per-use | Free (local) |
| Privacy | Cloud-based | Local processing |
| Detail Levels | Low/High | Standard |
| Language Support | Extensive | Good |

### OpenAI Provider
- **Best for**: Production applications requiring high accuracy
- **Default Model**: `gpt-4o`
- **Features**: Detail level control, excellent text recognition

### Ollama Provider (Local)
- **Best for**: Privacy-sensitive applications or development
- **Default Model**: `llava`
- **Features**: Local processing, no API costs, offline capability

## Advanced Usage

### Custom Vision Service

```typescript
import { VisionService, VisionConfig } from '@astreus-ai/astreus';

// Create custom vision service with configuration
const config: VisionConfig = {
  provider: 'openai',
  model: 'gpt-4o',
  apiKey: 'your-key',
  baseURL: 'https://api.openai.com/v1'  // Optional
};

const visionService = new VisionService(config);

// Direct service usage
const analysis: string = await visionService.analyzeImage('/path/to/image.jpg', {
  prompt: 'Perform detailed medical image analysis',
  maxTokens: 2000,
  detail: 'high'
});
```

### Batch Processing

```typescript
const images = [
  '/path/to/image1.jpg',
  '/path/to/image2.png',
  '/path/to/image3.gif'
];

const results = await Promise.all(
  images.map(imagePath => 
    agent.describeImage(imagePath, 'concise')
  )
);

console.log('Analysis results:', results);
```


## Built-in Vision Tools

When vision is enabled, these tools are automatically available:

### analyze_image
- **Parameters**: 
  - `image_path` (string, required): Path to image file
  - `prompt` (string, optional): Custom analysis prompt
  - `detail` (string, optional): 'low' or 'high' detail level

### describe_image
- **Parameters**:
  - `image_path` (string, required): Path to image file
  - `style` (string, optional): Description style ('detailed', 'concise', 'accessibility', 'technical')

### extract_text_from_image
- **Parameters**:
  - `image_path` (string, required): Path to image file
  - `language` (string, optional): Language hint for better OCR accuracy

